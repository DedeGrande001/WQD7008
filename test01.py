"""
Spark è½»é‡æµ‹è¯• - æœ€ç»ˆä¿®æ­£ç‰ˆ
è§£å†³ Foreign Key æŠ¥é”™ï¼šåŠ å…¥â€œå›è¯» IDâ€é€»è¾‘
"""
import os
import traceback
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, lit, desc, current_timestamp

print("=" * 70)
print("ğŸ§ª Spark è½»é‡æµ‹è¯• (Final Fix) - å¤„ç† 100 æ¡æ•°æ®")
print("=" * 70)

# ---------------------------------------------------------
# 1. é…ç½®éƒ¨åˆ†
# ---------------------------------------------------------
RDS_HOST = "recommendation-db.croqeqgd3egv.us-east-1.rds.amazonaws.com"
RDS_DB = "recommendation_db"
RDS_USER = "admin"
RDS_PASSWORD = "RecommendDB2026!"
# å¿…é¡»å…è®¸å…¬é’¥æ£€ç´¢ï¼Œå¦åˆ™æœ‰äº›ç‰ˆæœ¬è¿ä¸ä¸Š
JDBC_URL = f"jdbc:mysql://{RDS_HOST}:3306/{RDS_DB}?useSSL=false&allowPublicKeyRetrieval=true"

print(f"\næ•°æ®åº“: {RDS_HOST}/{RDS_DB}")

print("\nğŸ“¦ åˆ›å»º Spark Session...")
spark = SparkSession.builder \
    .appName("MovieLens-Mini-Test-Final") \
    .master("local[1]") \
    .config("spark.driver.memory", "512m") \
    .config("spark.executor.memory", "512m") \
    .config("spark.sql.shuffle.partitions", "2") \
    .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.33") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")
print("âœ… Spark Session åˆ›å»ºæˆåŠŸ")

try:
    # ---------------------------------------------------------
    # 2. è¯»å–æ•°æ® & è®¡ç®—
    # ---------------------------------------------------------
    print("\nğŸ“‚ è¯»å–æ•°æ®å¹¶è®¡ç®—...")
    
    # è¯»å– Movies (å‰100æ¡)
    movies_full = spark.read.csv("data/movies.csv", header=True, inferSchema=True)
    movies_df = movies_full.limit(100)
    
    # æå– ID åˆ—è¡¨
    movie_ids = [row.movieId for row in movies_df.select("movieId").collect()]
    
    # è¯»å– Ratings
    ratings_full = spark.read.csv("data/ratings.csv", header=True, inferSchema=True)
    ratings_df = ratings_full.filter(col("movieId").isin(movie_ids))

    # è®¡ç®—ç»Ÿè®¡
    movie_stats = ratings_df.groupBy("movieId").agg(
        avg("rating").alias("avg_rating"),
        count("rating").alias("rating_count")
    )

    # è´å¶æ–¯å¹³å‡
    avg_rating_all = ratings_df.agg(avg("rating")).first()[0] or 3.0
    m = 10
    C = avg_rating_all

    recommendations = movie_stats.withColumn(
        "recommendation_score",
        ((col("rating_count") / (col("rating_count") + lit(m))) * col("avg_rating") +
         (lit(m) / (col("rating_count") + lit(m))) * lit(C))
    )

    # ---------------------------------------------------------
    # 3. å†™å…¥ Movies è¡¨
    # ---------------------------------------------------------
    print(f"\nğŸ’¾ [1/3] å†™å…¥ movies è¡¨...")
    
    movies_ready = movies_df.join(movie_stats, "movieId", "left") \
        .na.fill({"avg_rating": 0.0, "rating_count": 0})

    movies_to_db = movies_ready.select(
        col("movieId").alias("movie_id"),      
        col("title"),                          
        col("genres"),                        
        col("avg_rating"),                     
        col("rating_count")                   
    ).withColumn("year", lit(None).cast("int")) \
     .withColumn("created_at", current_timestamp()) \
     .withColumn("updated_at", current_timestamp())

    # è¿™é‡Œçš„ append æ¨¡å¼ä¼šä¿ç•™ä¹‹å‰å†™å…¥çš„é‡å¤æ•°æ®ï¼Œæµ‹è¯•æ²¡å…³ç³»
    # ç”Ÿäº§ç¯å¢ƒé€šå¸¸éœ€è¦åš upsertï¼Œä½†è¿™é‡Œä¿æŒç®€å•
    movies_to_db.write.format("jdbc") \
        .option("url", JDBC_URL) \
        .option("dbtable", "movies") \
        .option("user", RDS_USER) \
        .option("password", RDS_PASSWORD) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .mode("append") \
        .save()
    
    print(f"   âœ… movies è¡¨å†™å…¥æˆåŠŸ")

    # ---------------------------------------------------------
    # 4. å›è¯» ID æ˜ å°„ (è§£å†³å¤–é”®é—®é¢˜çš„å…³é”®æ­¥éª¤)
    # ---------------------------------------------------------
    print(f"\nğŸ”„ [2/3] ä»æ•°æ®åº“å›è¯» ID æ˜ å°„...")
    
    # æˆ‘ä»¬åªè¯» id å’Œ movie_id ä¸¤åˆ—ï¼Œç”¨æ¥åšå­—å…¸æ˜ å°„
    db_movies_df = spark.read.format("jdbc") \
        .option("url", JDBC_URL) \
        .option("dbtable", "movies") \
        .option("user", RDS_USER) \
        .option("password", RDS_PASSWORD) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .load() \
        .select(col("id").alias("pk_id"), col("movie_id").alias("csv_id"))

    # å°†æ¨èç»“æœ (å« csv_id) ä¸ æ•°æ®åº“æ˜ å°„ (csv_id -> pk_id) è¿›è¡Œ Join
    # recommendations.movieId æ˜¯ CSV é‡Œçš„ ID
    # db_movies_df.csv_id ä¹Ÿæ˜¯ CSV é‡Œçš„ ID
    # db_movies_df.pk_id æ˜¯ MySQL ç”Ÿæˆçš„ä¸»é”® ID (æˆ‘ä»¬è¦å†™å…¥å¤–é”®çš„å€¼)
    
    final_rec_data = recommendations.join(
        db_movies_df, 
        recommendations.movieId == db_movies_df.csv_id, 
        "inner"
    )
    
    print(f"   âœ… æ˜ å°„æˆåŠŸï¼Œå‡†å¤‡å†™å…¥ {final_rec_data.count()} æ¡æ¨èæ•°æ®")

    # ---------------------------------------------------------
    # 5. å†™å…¥ Recommendation Data è¡¨
    # ---------------------------------------------------------
    print(f"\nğŸ’¾ [3/3] å†™å…¥ recommendation_data è¡¨...")

    rec_to_db = final_rec_data.select(
        col("pk_id").alias("movie_id"),          # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯æ•°æ®åº“çš„ä¸»é”® IDï¼
        col("recommendation_score")
    ).withColumn("popularity_score", lit(0.0)) \
     .withColumn("genre_match_score", lit(0.0)) \
     .withColumn("user_id", lit(None).cast("int")) \
     .withColumn("created_at", current_timestamp()) \
     .withColumn("updated_at", current_timestamp())

    rec_to_db.write.format("jdbc") \
        .option("url", JDBC_URL) \
        .option("dbtable", "recommendation_data") \
        .option("user", RDS_USER) \
        .option("password", RDS_PASSWORD) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .mode("append") \
        .save()

    print(f"   âœ… recommendation_data è¡¨å†™å…¥æˆåŠŸ")

    print("\n" + "=" * 70)
    print("ğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨å®Œæˆï¼æ•°æ®é€»è¾‘å·²ä¿®å¤ã€‚")
    print("=" * 70)

except Exception as e:
    print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {type(e).__name__}")
    print(f"   è¯¦ç»†ä¿¡æ¯: {str(e)}")
    # æ‰“å°ç®€çŸ­çš„é”™è¯¯é“¾ï¼Œæ–¹ä¾¿è°ƒè¯•
    traceback.print_exc()

finally:
    if 'spark' in locals():
        spark.stop()
        print("\nğŸ›‘ Spark Session å·²åœæ­¢")