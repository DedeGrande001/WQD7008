"""
æµ‹è¯•ä» S3 è¯»å–æ•°æ®çš„è„šæœ¬
ç”¨äºåœ¨æœ¬åœ°è°ƒè¯• Spark S3 é…ç½®
"""
import os
from pyspark.sql import SparkSession

print("=" * 60)
print("æµ‹è¯• Spark è¯»å– S3 æ•°æ®")
print("=" * 60)

# S3 è·¯å¾„
S3_BUCKET = "s3a://recommendation-system-data-dedegrande/input"

print(f"\nå°è¯•è¯»å–: {S3_BUCKET}/movies.csv")

# å°è¯•æ–¹æ¡ˆ 1: åŸºç¡€é…ç½®
print("\næ–¹æ¡ˆ 1: ä½¿ç”¨ Hadoop AWS 3.3.2 + AWS SDK 1.11.1026")
try:
    spark = SparkSession.builder \
        .appName("S3-Test") \
        .master("local[*]") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    print("âœ… Spark session åˆ›å»ºæˆåŠŸ")

    # å°è¯•è¯»å–
    print(f"ğŸ“‚ è¯»å– movies.csv...")
    movies_df = spark.read.csv(
        f"{S3_BUCKET}/movies.csv",
        header=True,
        inferSchema=True
    )

    count = movies_df.count()
    print(f"âœ… æˆåŠŸè¯»å– {count} æ¡ç”µå½±è®°å½•")

    # æ˜¾ç¤ºå‰å‡ è¡Œ
    print("\nå‰ 5 æ¡è®°å½•:")
    movies_df.show(5, truncate=False)

    spark.stop()
    print("\nâœ…âœ…âœ… æ–¹æ¡ˆ 1 æˆåŠŸï¼")

except Exception as e:
    print(f"\nâŒ æ–¹æ¡ˆ 1 å¤±è´¥: {type(e).__name__}")
    print(f"é”™è¯¯: {str(e)[:200]}")
    try:
        spark.stop()
    except:
        pass

print("\n" + "=" * 60)
