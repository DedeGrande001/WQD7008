"""
Spark è½»é‡æµ‹è¯• - åªå¤„ç†å°‘é‡æ•°æ®éªŒè¯æµç¨‹
"""
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, lit, desc

print("=" * 70)
print("ğŸ§ª Spark è½»é‡æµ‹è¯• - å¤„ç† 100 æ¡æ•°æ®")
print("=" * 70)

# æ•°æ®åº“é…ç½®
RDS_HOST = "recommendation-db.croqeqgd3egv.us-east-1.rds.amazonaws.com"
RDS_DB = "recommendation_db"
RDS_USER = "admin"
RDS_PASSWORD = "RecommendDB2026!"
JDBC_URL = f"jdbc:mysql://{RDS_HOST}:3306/{RDS_DB}"

print(f"\næ•°æ®åº“: {RDS_HOST}/{RDS_DB}")

# åˆ›å»º Spark Sessionï¼ˆé™åˆ¶èµ„æºï¼‰
print("\nğŸ“¦ åˆ›å»º Spark Session (limited resources)...")
spark = SparkSession.builder \
    .appName("MovieLens-Mini-Test") \
    .master("local[1]") \
    .config("spark.driver.memory", "512m") \
    .config("spark.executor.memory", "512m") \
    .config("spark.sql.shuffle.partitions", "2") \
    .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.33") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")
print("âœ… Spark Session åˆ›å»ºæˆåŠŸ")

try:
    # 1. è¯»å– Movies (å‰100æ¡)
    print("\nğŸ“‚ è¯»å– movies.csv (å‰100æ¡)...")
    movies_full = spark.read.csv("data/movies.csv", header=True, inferSchema=True)
    movies_df = movies_full.limit(100)
    movies_count = movies_df.count()
    print(f"âœ… åŠ è½½ {movies_count} éƒ¨ç”µå½±")

    # è·å–è¿™äº›ç”µå½±çš„ ID
    movie_ids = [row.movieId for row in movies_df.select("movieId").collect()]
    print(f"   ç”µå½±IDèŒƒå›´: {min(movie_ids)} - {max(movie_ids)}")

    # 2. è¯»å– Ratings (åªè¯»å–è¿™100éƒ¨ç”µå½±çš„è¯„åˆ†)
    print(f"\nğŸ“‚ è¯»å– ratings.csv (åªè¯»å–è¿™{movies_count}éƒ¨ç”µå½±çš„è¯„åˆ†)...")
    ratings_full = spark.read.csv("data/ratings.csv", header=True, inferSchema=True)
    ratings_df = ratings_full.filter(col("movieId").isin(movie_ids))
    ratings_count = ratings_df.count()
    print(f"âœ… åŠ è½½ {ratings_count} æ¡è¯„åˆ†")

    # 3. è®¡ç®—æ¨èæ•°æ®
    print(f"\nğŸ”¢ è®¡ç®—æ¨èåˆ†æ•°...")
    movie_stats = ratings_df.groupBy("movieId").agg(
        avg("rating").alias("avg_rating"),
        count("rating").alias("rating_count")
    )

    # ä½¿ç”¨è´å¶æ–¯åŠ æƒè¯„åˆ†
    total_ratings = ratings_df.count()
    avg_rating_all = ratings_df.agg(avg("rating")).first()[0]
    m = 10  # æœ€å°è¯„åˆ†æ•°é˜ˆå€¼
    C = avg_rating_all  # å…¨å±€å¹³å‡åˆ†

    print(f"   å…¨å±€å¹³å‡åˆ†: {C:.2f}")
    print(f"   æ€»è¯„åˆ†æ•°: {total_ratings}")

    recommendations = movie_stats.withColumn(
        "recommendation_score",
        ((col("rating_count") / (col("rating_count") + lit(m))) * col("avg_rating") +
         (lit(m) / (col("rating_count") + lit(m))) * lit(C))
    )

    rec_count = recommendations.count()
    print(f"âœ… ç”Ÿæˆ {rec_count} æ¡æ¨è")

    # 4. Join ç”µå½±ä¿¡æ¯
    print(f"\nğŸ”— åˆå¹¶ç”µå½±ä¿¡æ¯...")
    final_data = recommendations.join(movies_df, "movieId", "inner") \
        .select(
            col("movieId").alias("movie_id"),
            col("title"),
            col("genres"),
            col("avg_rating"),
            col("rating_count"),
            col("recommendation_score")
        ) \
        .orderBy(desc("recommendation_score"))

    # æ˜¾ç¤º Top 10
    print(f"\nğŸ† Top 10 æ¨è:")
    final_data.show(10, truncate=False)

    # 5. å†™å…¥æ•°æ®åº“
    print(f"\nğŸ’¾ å†™å…¥æ•°æ®åº“...")

    # å†™å…¥ movies è¡¨
    print(f"   å†™å…¥ movies è¡¨...")
    movies_to_db = movies_df.select(
        col("movieId").alias("movie_id"),
        col("title"),
        col("genres")
    )

    movies_to_db.write.format("jdbc") \
        .option("url", JDBC_URL) \
        .option("dbtable", "movies") \
        .option("user", RDS_USER) \
        .option("password", RDS_PASSWORD) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .mode("append") \
        .save()

    print(f"   âœ… {movies_count} éƒ¨ç”µå½±å·²å†™å…¥")

    # å†™å…¥ recommendation_data è¡¨
    print(f"   å†™å…¥ recommendation_data è¡¨...")
    rec_to_db = final_data.select(
        col("movie_id"),
        col("recommendation_score")
    )

    rec_to_db.write.format("jdbc") \
        .option("url", JDBC_URL) \
        .option("dbtable", "recommendation_data") \
        .option("user", RDS_USER) \
        .option("password", RDS_PASSWORD) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .mode("append") \
        .save()

    print(f"   âœ… {rec_count} æ¡æ¨èå·²å†™å…¥")

    print("\n" + "=" * 70)
    print("âœ…âœ…âœ… æµ‹è¯•å®Œæˆï¼æ•°æ®å·²æˆåŠŸå†™å…¥ RDS")
    print("=" * 70)

except Exception as e:
    print(f"\nâŒ é”™è¯¯: {type(e).__name__}")
    print(f"   {str(e)}")
    import traceback
    traceback.print_exc()

finally:
    spark.stop()
    print("\nğŸ›‘ Spark Session å·²åœæ­¢")
