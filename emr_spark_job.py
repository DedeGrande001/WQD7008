"""
EMR Spark Job - Process MovieLens data and write to RDS
"""
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, regexp_extract, when, lit, desc
from pyspark.sql.types import IntegerType

# RDS é…ç½®ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è·å–ï¼‰
if len(sys.argv) < 6:
    print("Usage: spark-submit emr_spark_job.py <s3_input> <rds_host> <rds_db> <rds_user> <rds_password>")
    sys.exit(1)

S3_INPUT = sys.argv[1]  # s3://bucket/input/
RDS_HOST = sys.argv[2]
RDS_DB = sys.argv[3]
RDS_USER = sys.argv[4]
RDS_PASSWORD = sys.argv[5]

JDBC_URL = f"jdbc:mysql://{RDS_HOST}:3306/{RDS_DB}"

print(f"Starting Spark job...")
print(f"S3 Input: {S3_INPUT}")
print(f"RDS Host: {RDS_HOST}")

# åˆ›å»º Spark Session
spark = SparkSession.builder \
    .appName("MovieLens-Recommendation-EMR") \
    .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.33,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.timeout", "60000") \
    .config("spark.hadoop.fs.s3a.connection.establish.timeout", "60000") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark session created")

# 1. è¯»å–æ•°æ®
print("\nğŸ“‚ Reading movies from S3...")
movies_df = spark.read.csv(
    f"{S3_INPUT}/movies.csv",
    header=True,
    inferSchema=True
)
print(f"âœ… Loaded {movies_df.count():,} movies")

print("\nğŸ“‚ Reading ratings from S3...")
ratings_df = spark.read.csv(
    f"{S3_INPUT}/ratings.csv",
    header=True,
    inferSchema=True
)
print(f"âœ… Loaded {ratings_df.count():,} ratings")

# 2. æå–å¹´ä»½
print("\nğŸ“Š Processing data...")
movies_df = movies_df.withColumn(
    "year",
    when(
        regexp_extract(col("title"), r"\((\d{4})\)", 1) != "",
        regexp_extract(col("title"), r"\((\d{4})\)", 1).cast(IntegerType())
    ).otherwise(None)
)

# 3. è®¡ç®—è¯„åˆ†ç»Ÿè®¡
rating_stats = ratings_df.groupBy("movieId").agg(
    avg("rating").alias("avg_rating"),
    count("rating").alias("rating_count")
)

# 4. è¿‡æ»¤å¹¶ç”Ÿæˆæ¨è
MIN_RATINGS = 100
qualified_movies = rating_stats.filter(col("rating_count") >= MIN_RATINGS)
print(f"âœ… Found {qualified_movies.count():,} qualified movies")

recommendations = qualified_movies.join(movies_df, on="movieId", how="inner")

# 5. è®¡ç®— Bayesian åŠ æƒè¯„åˆ†
global_stats = rating_stats.agg(
    avg("avg_rating").alias("global_avg")
).collect()[0]

C = global_stats["global_avg"]
m = MIN_RATINGS

recommendations = recommendations.withColumn(
    "recommendation_score",
    ((col("rating_count") / (col("rating_count") + lit(m))) * col("avg_rating") +
     (lit(m) / (col("rating_count") + lit(m))) * lit(C))
).withColumn(
    "popularity_score",
    col("rating_count")
)

# 6. å‡†å¤‡å†™å…¥æ•°æ®åº“çš„æ•°æ®
db_data = recommendations.select(
    col("movieId").alias("movie_id"),
    col("title"),
    col("genres"),
    col("year"),
    col("avg_rating"),
    col("rating_count"),
    col("recommendation_score"),
    col("popularity_score")
)

print(f"\nâœ… Generated {db_data.count():,} recommendations")
print("\nğŸ¬ Top 10 Recommendations:")
db_data.orderBy(desc("recommendation_score")).select("title", "recommendation_score").show(10, False)

# 7. å†™å…¥ RDS
print(f"\nğŸ“Š Writing to RDS: {RDS_HOST}/{RDS_DB}...")

# å†™å…¥æ•°æ®ï¼ˆä½¿ç”¨ overwrite æ¨¡å¼æ¸…ç©ºæ—§æ•°æ®ï¼‰
db_data.write \
    .format("jdbc") \
    .option("url", JDBC_URL) \
    .option("dbtable", "movies") \
    .option("user", RDS_USER) \
    .option("password", RDS_PASSWORD) \
    .option("driver", "com.mysql.cj.jdbc.Driver") \
    .mode("overwrite") \
    .save()

print("âœ… Data written to RDS successfully!")

print("\n" + "="*70)
print("âœ… JOB COMPLETED SUCCESSFULLY!")
print("="*70)

spark.stop()
